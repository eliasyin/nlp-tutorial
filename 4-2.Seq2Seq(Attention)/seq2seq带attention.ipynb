{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bigger-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "union-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preliminary-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_class, n_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        \n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
    "        \n",
    "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
    "        enc_inputs = enc_inputs.transpose(0, 1)\n",
    "        dec_inputs = dec_inputs.transpose(0, 1)\n",
    "        \n",
    "        enc_inputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
    "        \n",
    "        trained_attn = []\n",
    "        hidden = enc_hidden\n",
    "        n_step = len(dec_inputs)\n",
    "        model = torch.empty([n_step, 1, n_class])\n",
    "        \n",
    "        for i in range(n_step):\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
    "            atten_weights = self.get_att_weight(dec_output, enc_outputs)\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "            \n",
    "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
    "            dec_output = dec_output.squeeze(0)\n",
    "            context = context.squeeze(1)\n",
    "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
    "        \n",
    "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
    "    \n",
    "    def get_att_weight(self, dec_output, enc_outputs):\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = torch.zeros(n_step)\n",
    "        \n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
    "            \n",
    "        return F.softmax(attn_scores).view(1, 1, -1)\n",
    "    \n",
    "    def get_att_score(self, dec_output, enc_output):\n",
    "        score = self.attn(enc_output)\n",
    "        return torch.dot(dec_output.view(-1), score.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5\n",
    "n_hidden = 128\n",
    "\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i,w in enumerate(word_list)}\n",
    "number_dict = {i: w for i,w in enumerate(word_list) }\n",
    "n_class = len(word_dict)\n",
    "\n",
    "hidden = torch.zeros(1, 1, n_hidden)\n",
    "\n",
    "model = Attention(n_class, n_hidden)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    output, __ = model(input_batch, hidden, output_batch)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
